{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a140c4a3",
      "metadata": {
        "id": "a140c4a3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e600740c",
      "metadata": {
        "id": "e600740c"
      },
      "outputs": [],
      "source": [
        "model_id = \"ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a8e54c",
      "metadata": {
        "id": "31a8e54c"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IibKiznvTXXS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "923efec8205643e492411c917ab851dc",
            "ea595d6ed0b346ee98a42b3643417d0a",
            "61288a9864624b248b702398381f148b",
            "688c6a3293f444d2860d6cef0167e5c4",
            "d883095a79984539831eee23750c7df3",
            "fd2c946aea51499b9c3dcaa677d8cadc",
            "981f3ba4e41e48c5a6b13fffbd727608",
            "a5e3976a060c4f389e98379c609c4b4f",
            "8b68063f90ce4d15b9bbc4927b737b8a",
            "2a2fbfda0fb5438fa88248b82a2b3176",
            "1d03decdc22b41bfa122e172226ee151"
          ]
        },
        "id": "IibKiznvTXXS",
        "outputId": "b5d3ec6a-6046-4d8d-b63f-83592d71cd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model 'ytu-ce-cosmos/Turkish-Gemma-9b-v0.1' Hugging Face Hub'dan indiriliyor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "923efec8205643e492411c917ab851dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model indirildi.\n",
            "Model '/content/drive/MyDrive/LLM_Modellerim/Turkish-Gemma-9b-v0.1' konumuna kaydediliyor...\n",
            "Model Drive'a kaydedildi.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "save_path_on_drive = \"/content/drive/MyDrive/LLM_Modellerim/Turkish-Gemma-9b-v0.1\" \n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config, \n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        "\n",
        ")\n",
        "\n",
        "model.generation_config.do_sample = True # Örneklemeyi etkinleştirin\n",
        "model.generation_config.temperature = 0.7 # Kendi istediğiniz bir değerde bırakabilirsiniz\n",
        "model.generation_config.top_p = 0.9     # Kendi istediğiniz bir değerde bırakabilirsiniz\n",
        "model.generation_config.top_k = 50\n",
        "# Modeli ve tokenizer'ı Drive'a kaydedin\n",
        "# Önce klasörü oluşturduğunuzdan emin olun\n",
        "import os\n",
        "os.makedirs(save_path_on_drive, exist_ok=True)\n",
        "\n",
        "print(f\"Model '{save_path_on_drive}' konumuna kaydediliyor...\")\n",
        "model.save_pretrained(save_path_on_drive)\n",
        "tokenizer.save_pretrained(save_path_on_drive)\n",
        "print(\"Model Drive'a kaydedildi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d370c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "9f931d479f6d450a9563eb8bb87f51b1",
            "19a58b842e854a2bafbf2988e7fb029e",
            "595dc2bc7988426db07e1152b4d770fc",
            "5a473e235eb14a64848253d9f4b6585c",
            "9a00640ddd2b452fbf2268783e75a5be",
            "12737af83f0c4b76b95d3471053683d2",
            "e5e146623b174392ba4d96a7135a4134",
            "f13dd108470942e59f095312c4853209",
            "9cba151bc9f4419bb98a93e8046a1fa7",
            "eff2fd909bca480994f2c9742af295de",
            "721b0b9afae747988582aaf1cafce858"
          ]
        },
        "id": "21d370c8",
        "outputId": "d081a76f-87e9-44c8-ad42-ae9cd2a9bf60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f931d479f6d450a9563eb8bb87f51b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation='eager',\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0c4925",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd0c4925",
        "outputId": "cf1ebb83-b243-425b-b3e7-0d7c44f79a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 864,288,768 || all params: 10,105,994,752 || trainable%: 8.5522\n"
          ]
        }
      ],
      "source": [
        "# 3. LoRA Konfigürasyonu\n",
        "peft_config = LoraConfig(\n",
        "    r=256,\n",
        "    lora_alpha=128,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9057ce",
      "metadata": {
        "id": "aa9057ce"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"datas.jsonl\"\n",
        "data = []\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NoyfnADpi7wi",
      "metadata": {
        "id": "NoyfnADpi7wi"
      },
      "source": [
        "# Yeni Bölüm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9506eeeb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0cf10dab79d0426c884598f9d8161b2a",
            "6113a9cf5e6d424296f6e7c08ba89827",
            "7c49316503bf4cbd807c26bd8456cd0d",
            "3d44a09cfee946eeb4fe4217e95da1db",
            "a1e7eca734314df7bce1a256f1abf03b",
            "ba4e914cba794e4d89dd20dce52a6e4d",
            "319d513ae15f405bb58b2d224844ffdb",
            "3b99eaf963914776a1ca7245aef17bfb",
            "71191d6467de4c72a11af1cef57be38a",
            "2311b2b8376040db9f9af748e803b89e",
            "e232b91c4d1541ff8c087546bea65176"
          ]
        },
        "id": "9506eeeb",
        "outputId": "ba40794a-25af-43ee-a072-d68734283d3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cf10dab79d0426c884598f9d8161b2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/766 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize(example):\n",
        "    messages = example[\"messages\"]\n",
        "\n",
        "    formatted_messages = []\n",
        "    for message in messages:\n",
        "        formatted_messages.append({\n",
        "            \"role\": message[\"role\"],\n",
        "            \"content\": message[\"content\"]\n",
        "        })\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        formatted_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03895c73",
      "metadata": {
        "id": "03895c73"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f21019",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "32f21019",
        "outputId": "a472d378-b0a7-45f8-c571-1d6898e391c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [288/288 41:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.104800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.989100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.936200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.838700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.843400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.707800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.768000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.412600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.447400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.425100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.430700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.447900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.334900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.290600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.282100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.262900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.284600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=288, training_loss=0.5608267693056, metrics={'train_runtime': 2508.4571, 'train_samples_per_second': 0.916, 'train_steps_per_second': 0.115, 'total_flos': 6.486574617015091e+16, 'train_loss': 0.5608267693056, 'epoch': 3.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./turkish-gemma-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c593c54f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c593c54f",
        "outputId": "2ba3e6ef-3b93-4b48-d1eb-5cef7abb81eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./turkish-gemma-lora/tokenizer_config.json',\n",
              " './turkish-gemma-lora/special_tokens_map.json',\n",
              " './turkish-gemma-lora/chat_template.jinja',\n",
              " './turkish-gemma-lora/tokenizer.model',\n",
              " './turkish-gemma-lora/added_tokens.json',\n",
              " './turkish-gemma-lora/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./turkish-gemma-lora\")\n",
        "tokenizer.save_pretrained(\"./turkish-gemma-lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4o3OtuLXVU0n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o3OtuLXVU0n",
        "outputId": "a0da6a9c-f832-413f-e005-0b882c3a8d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}